## Version note
1 SGD 2000 不收敛 loss2.03

2 Adam 2000 loss 2.00

3 Adam 10000 loss 1.80 大概在5000时就保持不变 -> 测试 22% test1.csv

4 Adam 初始化 5000 loss 360+ 初始化不好爆炸了

5 Adam 初始化 5000 只使用边的关系

6 Adam 初始化 5000 优化了特征提取（数量级） -> 测试7% test3.csv

7 Adam 乱拼结构 10000 lr 0.0001

8 Adam 乱拼结构 10000 lr 0.001 loss 1.0 -> 测试16% test4.csv

9 Adam 乱拼结构 10000 lr 0.001 隐藏层250维度 loss 1.0 0.97 0.94 0.90 0.87 0.87 

选取 4999 做输出 test7 -> 测试 27%
选取 1999 做输出 test8

优化输出4999 test9 -> 测试 29%

10 Adam 乱拼结构 10000 lr 0.0001 隐藏层250维度 meta feature

2999 loss 0.001 test10 感觉看上去不太好 但其实很好! -> 测试42.2%!

1999 test10_1

999 test10_2

11 同上，但完整训练版本

4999 loss 0.0001 test11 -> 测试41%

5999 loss 8.e-5

6999 loss 4.e-5

7999 loss 2.e-5

12 只使用metapath生成特征 (paper->author->paper, paper->paper) 128 维

999 loss 0.0003 test12 -> 测试42.3% 存在过拟合风险

13 同上， 但修改metapath生成特征维度为256，并修改隐藏层大小300

499 loss 0.005 test13 -> 测试44.6%

14 生成路径时使用50 50 配置，生成特征维度512，并修改隐藏层大小1024

99 0.09 -> test17 测试47.065%

199 0.002 test14 -> 测试46.5%

299 0.0003 test16 -> 测试46.2%

15 metapath 10 5 配置，生成特征维度512，隐藏层1024

199 0.02

299 0.007 test15 -> 测试44.5%

16 细粒度重新训练14

17 添加ABA BAB 两种路径后的feature

99 0.35 test18 -> 测试46.2%

18 ABA BAB 20 10 20 10 

99 0.2loss-> 测试45.5%

19 添加了年份信息

169 loss 0.01 test20 -> 测试45.9%

20 100 100 100 50 版本

149 loss 0.01 test21 -> 测试46.7%

99 test22 -> 测试46.8% 说明特征饱和：在50 50 参数设置下

49 test23 -> 测试46.1%

21 学习其他分类器，优化网络结构 (50 50) 网络结构 512->120->84->10 

299 0,1 399 0,02 499 0,004

399 test24 -> 43.3% 499 test25 -> 42.5%

(妈的，是我蠢了) 改成batchsize8 再试一次

59 0.07 test26 -> 42.5%

22 加 drop 的 lenet版本(不看好)

499 0.09 -> 44.0%

23 加 drop 的 1024版本

329 0.01 -> 46.5%

499 -> 46.1%

259 -> 46.6%

24 数据预处理 失败！训错了

25 测试

原数据预处理，实验数据不预处理， 居然还高一点？

26 用预处理做增广 -> 47.8% test33

27 ml(中等大型feature生成)

28 ML + no augmentation

29 ABA BAB 预处理

30 ABC 预处理

31 128维 AB

网络不够大时loss下降不是很快 

改成512

31 1024

339-> 46.1%!!!

32 256 1024

33 512 2048

34 512 1024 1024 10

35 AB 512 2048